{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Objectives \n",
    "\n",
    "- Provide overview of Reinforcement Learning \n",
    "- Describe Markov Decision Processes (MDPs) \n",
    "- Implement Value and Policy Iteration \n",
    "- Actor-Critic Methods \n",
    "- Case Study: Optimizing Tax Collections with Constrained Value Iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is the _computational_ approach to learning from interaction (Sutton & Barto). Autonomous agents performing goal-oriented learning based on experience is the holy-grail of AI. Reinforcement Learning has a rich history with many successes in variety of different application areas. Before 2012, Reinforcement Learning achieved big wins in areas where you can model the dynamics of the underlying system well and subsequently apply planning algorithms to do the ultimate optimization. Pre-2012 wins in RL include: \n",
    "\n",
    "- **[[1992] TD-Gammon](http://www.bkgm.com/articles/tesauro/tdl.html)**: Tesauro et al. trained a multi-layer perceptron with Temporal Difference Learning and Self-Play  to beat the world's best Backgammon players. \n",
    "\n",
    "- **[[2004] Autonomous Helicopter Aerobatics through Apprenticeship Learning](https://cs.stanford.edu/~acoates/papers/AbbeelCoatesNg_IJRR2010.pdf)**: Ng et al. learnt the _dynamics of the model_ through expert flight trajectories and then applied an optimal control and dynamic programming to achieve state of the art in aerobatic helicopter flight. \n",
    "\n",
    "- **[[2011] Optimizing Tax Collections Constrained Reinforcement Learning](https://pdfs.semanticscholar.org/ddb1/fab21ebf37b1eb71c90844ec273d38ebf1bc.pdf)**: Melville et al. optimized tax collections for the New York state government leading to \\$120-150 Million in savings for the state over the course of three years. \n",
    "--- \n",
    "More recently, state of the art in Reinforcement Learning is leveraging recent advances in Deep Neural Networks which has created a new field called Deep Reinforcement Learning. \n",
    "<p>\n",
    "<img src=https://spectrum.ieee.org/image/MjczNjQ4NA.gif>\n",
    "<center> <em> Distributed Data Collection for Robots Learning to Grasp </em> </center>\n",
    "</p> \n",
    "\n",
    "- **[[2013] Human-level performance on many Atari games](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)**: Leveraging Convolutional Neural Networks and Q-Learning to surpass human performance in many Atari games. \n",
    "- **[[2016] AlphaGo and its successors](https://deepmind.com/research/alphago/)**: Probably the most famous result in modern Deep RL where Silver et al. beat world's best players in Go with MCTS and Neural Networks to represent the value function. \n",
    "- **[[2018] Learning Grasping and Dexterity](https://arxiv.org/abs/1806.10293)**: Recently, team from Google Brain released QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[![IMAGE_ALT_TEXT](http://opexanalytics.com/cnt/uploads/2017/11/LSMW_1.jpg)](https://www.youtube.com/watch?time_continue=8&v=gQa6iWGcGWY)\n",
    "<center> <em> Optimizing Supply Chains with DQN </em> </center> \n",
    "\n",
    "Reinforcement Learning as a paradigm offers a lot of interesting avenues of applications. Despite these successes, industrial applications of RL outside of organizations with easy access to large-scale compute and software infrastructure remain sparse. Wide-spread applications of RL require more sample efficient algorithms and new software tools for doing distributed computing. Our goal with this tutorial is two-fold:\n",
    "\n",
    "> Provide a solid conceptual foundation for understanding and evaluating existing state of the art RL methods (both strengths and weaknesses).\n",
    "\n",
    "> Showcase Ray, an emerging distributed execution framework, for experimenting with and deploying large-scale Reinforcement Learning algorithms.\n",
    "\n",
    "We believe that RL works well on applied problems where: 1.) Gathering simulations and experience is cheap, e.g. games, narrow-domain robotics; 2.) Modelers have access to a compact representation of the environment dynamics to apply approximate dynamic programming.\n",
    "\n",
    "\n",
    "--- \n",
    "### Markov Decision Processes (MDPs)\n",
    "<img src=https://cdn-images-1.medium.com/max/1600/1*3NziBtrANN6UVltplxwaGA.png>\n",
    "<center> <em> <a href=http://pavel.surmenok.com/wp-content/uploads/2017/08/contextual-bandits-1.png> Source </em> </center>\n",
    "\n",
    "Reinforcement Learning involves **agents taking actions in non-stationary environments that maximize the total sum of discounted future rewards.** This likewise gives rise to **Markov Decision Processes** which consists of: \n",
    "\n",
    "- *States*: $S$\n",
    "- *Actions*: $A$\n",
    "- *Transition Probabilities*: $T(s' | s, a)$\n",
    "- *A scalar value Reward Function*: $R(s, a)$ \n",
    "- *$\\gamma$*: Discount Factor which generally represents how much patience an agent should have in an environment. \n",
    "\n",
    "Note: The Markov Assumption in *M*DP comes from the fact that:\n",
    "$$P[S_{t+1} | S_{t}] = P[S_{t+1} | S_1, ... , S_t]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at an example MDP..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
      "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
      "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
      "    If you step into one of those holes, you'll fall into the freezing water.\n",
      "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
      "    you navigate across the lake and retrieve the disc.\n",
      "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
      "    The surface is described using a grid like the following\n",
      "\n",
      "        SFFF\n",
      "        FHFH\n",
      "        FFFH\n",
      "        HFFG\n",
      "\n",
      "    S : starting point, safe\n",
      "    F : frozen surface, safe\n",
      "    H : hole, fall to your doom\n",
      "    G : goal, where the frisbee is located\n",
      "\n",
      "    The episode ends when you reach the goal or fall in a hole.\n",
      "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(FrozenLakeEnv.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for Understanding...**\n",
    "\n",
    "1. How many states does the `FrozenLake` enviornment have?\n",
    "2. What is the size of the action set? \n",
    "3. What is the reward? \n",
    "4. What will be the size of the transition probabilities? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True)],\n",
       " 2: [(0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False)]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing the MDP\n",
    "\n",
    "# accessing state 14\n",
    "env.env.P[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 13, 0.0, False),\n",
       " (0.3333333333333333, 14, 0.0, False),\n",
       " (0.3333333333333333, 15, 1.0, True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing outcome of action 1 in state 14\n",
    "env.env.P[14][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: This is a stochastic MDP. The first tuple represents going to state 13 by taking action 1 state 14 with probability 0.33 and achieving reward 0.0 without it being the terminal state.** In this toy example, how do we get to our **goal state**? We have access to the MDP and know the transition model so this just becomes a *simple planning problem*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration \n",
    "\n",
    "*Policy* $\\pi(a | s) $: Specifies which action to take in a given state $s$. \n",
    "\n",
    "*Value Function:* Input state $s$ and outputs the value of being in that state $s$ under a particular policy $\\pi$: $V^{\\pi}: S \\rightarrow \\mathbb{R}$\n",
    "\n",
    "<p>\n",
    "$$V^{\\pi}(s) = R^{\\pi}(s) + \\gamma \\Sigma_{s' \\in S} T(s' | s, \\pi(a | s))V^{\\pi}(s')$$\n",
    "</p>\n",
    "\n",
    "Note the recursive update. This can be represented as: \n",
    "<p>\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "           V^{\\pi}(s_1) \\\\\n",
    "           V^{\\pi}(s_2)  \\\\\n",
    "           \\vdots \\\\\n",
    "           V^{\\pi}(s_n)\n",
    "         \\end{bmatrix} = \\begin{bmatrix}\n",
    "           R^{\\pi}(s_1) \\\\\n",
    "           R^{\\pi}(s_2)  \\\\\n",
    "           \\vdots \\\\\n",
    "           R^{\\pi}(s_n)\n",
    "         \\end{bmatrix} + \\gamma \\begin{bmatrix}\n",
    "    T(s_1, | s_1, \\pi(s_1))       & T(s_2, | s_1, \\pi(s_1)) & \\dots & T(s_n, | s_1, \\pi(s_1)) \\\\\n",
    "    T(s_1, | s_2, \\pi(s_2))       & T(s_2, | s_2, \\pi(s_2)) & \\dots & T(s_n, | s_2, \\pi(s_1)) \\\\\n",
    "    \\vdots \\ddots \\\\\n",
    "    T(s_1, | s_n, \\pi(s_n))       & T(s_2, | s_n, \\pi(s_n)) & \\dots & T(s_n, | s_n, \\pi(s_n)) \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "           V^{\\pi}(s_1) \\\\\n",
    "           V^{\\pi}(s_2)  \\\\\n",
    "           \\vdots \\\\\n",
    "           V^{\\pi}(s_n)\n",
    "         \\end{bmatrix}\n",
    "$$\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    $$ V^\\pi = R^\\pi + \\gamma TV^\\pi $$ \n",
    "</p>\n",
    "\n",
    "\n",
    "Hence, we can derive an iterative algorithm for computing the value function: \n",
    "\n",
    "**Step 1**: Initialize $t=0$, $V^\\pi_0(s)$. \n",
    "\n",
    "**Step 2**: \n",
    "- Repeat Until Convergence. \n",
    "    - $V_t^\\pi = R^\\pi + \\gamma T^\\pi V_{t-1}^\\pi$\n",
    "\n",
    "Upon convergence, we can compute the optimal control policy: \n",
    "\n",
    "$$ \\pi^{*}(s) = \\arg\\max_{\\pi} $$ \n",
    "\n",
    "#### Intuition for Why This Converges \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MDP for the Frozen-Lake Enviornment \n",
    "S = np.arange(env.observation_space.n)\n",
    "A = np.arange(env.action_space.n)\n",
    "T = env.env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDP =  tuple\n",
    "V = np.array\n",
    "\n",
    "def value_iteration_frozen_lake(mdp: MDP, gamma: float, epsilon: float) -> V: \n",
    "\n",
    "    states, actions, transition_probabilities = mdp\n",
    "\n",
    "    # Initialize the value function \n",
    "    V = np.zeros(states.shape)\n",
    "    \n",
    "    # Repeat until convergence \n",
    "    while True: \n",
    "        Q = np.zeros((len(states), len(actions)), dtype=float)\n",
    "        V_old = V.copy()\n",
    "        for state in states: \n",
    "            for action in actions: \n",
    "                for probability, next_state, reward, done in transition_probabilities[state][action]:\n",
    "                    Q[state][action] += probability * (reward + gamma * V_old[next_state] * (not done))\n",
    "            V[state] = Q[state].max()\n",
    "        if np.sum(np.abs(V - V_old)) < epsilon:\n",
    "            break\n",
    "    return V\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Using matshow here just because it sets the ticks up nicely. imshow is faster.\n",
    "ax.imshow(value_iteration_frozen_lake((S, A, T), 0.99, 0.001).reshape(4, 4), cmap='seismic')\n",
    "\n",
    "for (i, j), z in np.ndenumerate(value_iteration_frozen_lake((S, A, T), 0.99, 0.001).reshape(4, 4)):\n",
    "    ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center', bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.539, 0.495, 0.466, 0.452],\n",
       "       [0.556, 0.   , 0.356, 0.   ],\n",
       "       [0.59 , 0.641, 0.614, 0.   ],\n",
       "       [0.   , 0.741, 0.862, 0.   ]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration_frozen_lake((S, A, T), 0.99, 0.001).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.539, 0.495, 0.466, 0.452, 0.556, 0.   , 0.356, 0.   , 0.59 ,\n",
       "       0.641, 0.614, 0.   , 0.   , 0.741, 0.862, 0.   ])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iterationa_frozen_lake((S, A, T), 0.99, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(S, A, P, gamma=.99, theta = 0.0000001):\n",
    " \n",
    "    V = np.random.random(len(S))\n",
    "    for i in range(100000):\n",
    "        old_V = V.copy()\n",
    "        \n",
    "        Q = np.zeros((len(S), len(A)), dtype=float)\n",
    "        for s in S:\n",
    "            for a in A:\n",
    "                for prob, s_prime, reward, done in P[s][a]:\n",
    "                    Q[s][a] += prob * (reward + gamma * old_V[s_prime] * (not done))\n",
    "            V[s] = Q[s].max()\n",
    "\n",
    "        if np.all(np.abs(old_V - V) < theta):\n",
    "            break\n",
    "    \n",
    "    pi = np.argmax(Q, axis=1)\n",
    "    return pi, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0]),\n",
       " array([0.542, 0.499, 0.471, 0.457, 0.558, 0.   , 0.358, 0.   , 0.592,\n",
       "        0.643, 0.615, 0.   , 0.   , 0.742, 0.863, 0.   ]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(S, A, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 15, 0, True)],\n",
       " 1: [(1.0, 15, 0, True)],\n",
       " 2: [(1.0, 15, 0, True)],\n",
       " 3: [(1.0, 15, 0, True)]}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what this looks like in code. \n",
    "convergence_epsilon = 0.001\n",
    "V = np.zeros_like(S)\n",
    "while True: \n",
    "    old_V = V.copy()\n",
    "    for s in S: \n",
    "        V[s] = \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
